{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kuiAj9WKY9Hl",
        "ATdtuZNqgxg9"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP489ziVasiODNwSeuAhliP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here we run sinkhorn on the palettes stored in drive to create our dataset for the neural network. This is basically labeling."
      ],
      "metadata": {
        "id": "I7J8o-NT4l7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "RrazqdQk64m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QwyGcUU24Jp-",
        "outputId": "5e3bb240-b7c9-43d0-87a4-4bd5075cb6d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Palettes already stored locally\n"
          ]
        }
      ],
      "source": [
        "# download palettes onto colab locally\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "palette_path = '/content/drive/MyDrive/Amortized Optimal Transport/Data/palettes.zip'\n",
        "local_path = '/content/palettes'\n",
        "\n",
        "if not os.path.exists(local_path):\n",
        "  print(\"Copying palettes zip...\")\n",
        "  shutil.copy(palette_path, '/content/palettes.zip')\n",
        "  print(\"Unzipping palettes zip...\")\n",
        "  !unzip -q /content/palettes.zip -d {local_path}\n",
        "  print(\"Done\")\n",
        "else:\n",
        "  print(\"Palettes already stored locally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Sinkhorn Algorithm Implementation\n",
        "\n",
        "First custom implementation of sinkhorn was computed directly. Led to a lot of underflow issues (understandably) so I worked in log space in my final impl."
      ],
      "metadata": {
        "id": "kuiAj9WKY9Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sinkhorn_naive_torch(a, b, cost_matrix, epsilon, max_iters=1000):\n",
        "  '''\n",
        "  Returns the optimal transport matrix of the sinkhorn algorithm.\n",
        "  Implemented naively without regard for underflow or other optimiziations.\n",
        "\n",
        "  Parameters:\n",
        "    a: left/source probability vector of shape (n,)\n",
        "    b: right/target probability vector of shape (m,)\n",
        "    cost_matrix: cost matrix in euclidian space of shape (n,m)\n",
        "    epsilon: smoothing parameter for the gibbs kernel\n",
        "\n",
        "  Returns:\n",
        "    (n,m) matrix depicting the optimal transport matrix from a to b\n",
        "  '''\n",
        "  gibbs_kernel = torch.exp(-cost_matrix/epsilon)\n",
        "  v = torch.ones_like(b)\n",
        "  past_v = v\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    u = a / (gibbs_kernel @ v)\n",
        "    v = b / (gibbs_kernel.T @ u)\n",
        "    if i % 10 == 0 and torch.allclose(v, past_v):\n",
        "      break\n",
        "    past_v = v\n",
        "\n",
        "  return u[:, None] * gibbs_kernel * v[None, :]"
      ],
      "metadata": {
        "id": "79T1QA3IeVYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Sinkhorn Implementation\n",
        "Note this is not identical to what is used in colab \"4. Training/Testing NN\"\n",
        "That implementation allows for warm starting sinkhorn with the Neural Net guess."
      ],
      "metadata": {
        "id": "ATdtuZNqgxg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sinkhorn(a, b, cost_matrix, epsilon, max_iters):\n",
        "  '''\n",
        "  My sinkhorn implementation.\n",
        "  Uses log probabilities to avoid underflow.\n",
        "\n",
        "  Parameters:\n",
        "    a: (n,) left/source probability vector\n",
        "    b: (m,) right/target probability vector\n",
        "    cost_matrix: (n,m) cost matrix in euclidean space\n",
        "    epsilon: smoothing parameter for the gibbs kernel\n",
        "\n",
        "  Returns:\n",
        "    (n,m) matrix depicting the optimal transport matrix from a to b\n",
        "  '''\n",
        "  M = cost_matrix # just renaming\n",
        "  log_v = torch.zeros_like(b)\n",
        "  past_log_v = log_v\n",
        "  log_a = torch.log(a)\n",
        "  log_b = torch.log(b)\n",
        "  log_eps = torch.log(torch.tensor(epsilon))\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    log_u = log_a - torch.logsumexp(-M/epsilon + log_v, 1)\n",
        "    log_v = log_b - torch.logsumexp(-M.T/epsilon + log_u, 1)\n",
        "\n",
        "    if torch.allclose(log_v, past_log_v, atol=1e-4, rtol=0):\n",
        "      break\n",
        "\n",
        "    past_log_v = log_v\n",
        "\n",
        "  return torch.exp(log_u[:, None] - M / epsilon + log_v[None, :])"
      ],
      "metadata": {
        "id": "KWoA921jkkVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title sinkhorn_batch(a, b, cost_matrix, epsilon, max_iters)\n",
        "def sinkhorn_batch(a, b, cost_matrix, epsilon=.1, max_iters=1000):\n",
        "  '''\n",
        "  My sinkhorn implementation (handles batches)\n",
        "  Uses log probabilities to avoid underflow.\n",
        "\n",
        "  Parameters:\n",
        "    a: (batch, n) left/source probability vector\n",
        "    b: (batch, m) right/target probability vector\n",
        "    cost_matrix: (batch, n, m) cost matrix in euclidean space\n",
        "    epsilon: smoothing parameter for the gibbs kernel\n",
        "\n",
        "  Returns:\n",
        "    (batch, n, m) matrix depicting the optimal transport matrix from a to b\n",
        "  '''\n",
        "  M = cost_matrix # just renaming\n",
        "  logK = -M/epsilon\n",
        "  logK_T = -M.permute(0,2,1)/epsilon\n",
        "\n",
        "  log_v = torch.zeros_like(b)\n",
        "  past_log_v = log_v\n",
        "  log_a = torch.log(a)\n",
        "  log_b = torch.log(b)\n",
        "  log_eps = torch.log(torch.tensor(epsilon))\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    log_u = log_a - torch.logsumexp(logK + log_v.unsqueeze(1), 2)\n",
        "    log_v = log_b - torch.logsumexp(logK_T + log_u.unsqueeze(1), 2)\n",
        "\n",
        "    if torch.allclose(log_v, past_log_v, atol=1e-4, rtol=0):\n",
        "      break\n",
        "\n",
        "    past_log_v = log_v\n",
        "\n",
        "  return torch.exp(log_u[:, :, None] - M / epsilon + log_v[:, None, :])"
      ],
      "metadata": {
        "id": "YWlvkCoZItYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating NN Training Data"
      ],
      "metadata": {
        "id": "nqmhGh1J7NO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensors from local storage\n",
        "\n",
        "palettes_dict = torch.load('/content/palettes/palette_bank.pt', map_location='cpu')\n",
        "\n",
        "centroids_tensor = palettes_dict['centroids'].cuda()\n",
        "weights_tensor = palettes_dict['weights'].cuda()\n",
        "# memberships_list = palettes_dict['memberships'] # unneeded here\n",
        "filenames_list = palettes_dict['filenames']\n",
        "\n",
        "print(centroids_tensor.shape)\n",
        "print(centroids_tensor)\n",
        "print(weights_tensor.shape)\n",
        "print(weights_tensor)\n",
        "print(filenames_list[0:10])\n",
        "print(filenames_list[-1:-11:-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YNcjl4vA7RSJ",
        "outputId": "76f87fc8-6599-48a6-dd69-0fa0157f9889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 128, 3])\n",
            "tensor([[[ 2.4221e+01, -1.3307e+01,  3.1794e+01],\n",
            "         [ 5.8568e+01, -2.7090e+01, -6.0873e-02],\n",
            "         [ 5.8039e+01, -1.7412e+01,  5.5339e+01],\n",
            "         ...,\n",
            "         [ 5.1207e+01, -1.9094e+01,  5.4930e+01],\n",
            "         [ 5.9679e+01, -1.8525e+01,  6.1443e+01],\n",
            "         [ 5.7916e+01, -2.2764e+00,  1.7729e+01]],\n",
            "\n",
            "        [[ 1.5576e+01, -7.8221e+00,  5.9351e+00],\n",
            "         [ 9.6874e+01,  9.7466e-01,  4.6768e+00],\n",
            "         [ 9.9080e+01, -1.8537e+00,  7.0578e+00],\n",
            "         ...,\n",
            "         [ 9.3113e+01,  2.9814e+00,  9.6789e+00],\n",
            "         [ 3.4186e+01, -7.3259e+00,  5.9161e+00],\n",
            "         [ 9.7258e+01,  1.3090e+00,  3.7129e+00]],\n",
            "\n",
            "        [[ 4.7509e+01,  2.4322e+01,  1.6912e+01],\n",
            "         [ 4.7459e+01,  3.1948e+01,  2.2976e+01],\n",
            "         [ 2.6430e+01,  7.6339e+00, -4.6345e+00],\n",
            "         ...,\n",
            "         [ 7.6952e+01,  1.2032e+01, -2.4740e+00],\n",
            "         [ 4.1159e+01,  2.5760e+01,  1.9359e+01],\n",
            "         [ 1.7052e+01, -7.7285e+00, -3.4121e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.7873e+01, -7.8681e+00,  7.3261e+00],\n",
            "         [ 5.6374e+01, -3.4549e+01,  5.3899e+01],\n",
            "         [ 4.8031e+01, -2.2662e+01,  2.7858e+01],\n",
            "         ...,\n",
            "         [ 7.8474e+01, -1.9276e+01,  2.9098e+01],\n",
            "         [ 5.3838e+01, -1.6799e+01,  3.1499e+01],\n",
            "         [ 4.3937e+01, -3.7745e+01,  4.7574e+01]],\n",
            "\n",
            "        [[ 1.1651e+01,  4.8294e+00,  1.0216e+01],\n",
            "         [ 8.9882e+00, -1.7338e+00,  3.3146e+00],\n",
            "         [ 1.5020e+01,  2.2805e+00,  6.1837e+00],\n",
            "         ...,\n",
            "         [ 5.6003e+00, -4.2996e-01,  1.0520e+00],\n",
            "         [ 1.4454e+01,  7.4735e-01,  4.5216e+00],\n",
            "         [ 9.0482e+00, -5.0264e-01,  1.2606e+00]],\n",
            "\n",
            "        [[ 7.7452e+01, -5.2440e+00, -5.3099e+00],\n",
            "         [ 1.1185e+01, -2.9043e-01,  8.3595e+00],\n",
            "         [ 3.8854e+01, -1.9360e+00,  2.9920e+00],\n",
            "         ...,\n",
            "         [ 5.5176e+01, -1.8942e+00,  3.7707e+00],\n",
            "         [ 4.9094e+01, -4.3764e+00, -5.3922e+00],\n",
            "         [ 4.0240e+01,  4.5401e+00,  6.2043e+00]]], device='cuda:0')\n",
            "torch.Size([2000, 128])\n",
            "tensor([[0.0097, 0.0061, 0.0063,  ..., 0.0140, 0.0153, 0.0011],\n",
            "        [0.0091, 0.0399, 0.0008,  ..., 0.0074, 0.0065, 0.0338],\n",
            "        [0.0088, 0.0050, 0.0026,  ..., 0.0128, 0.0066, 0.0116],\n",
            "        ...,\n",
            "        [0.0052, 0.0066, 0.0068,  ..., 0.0036, 0.0035, 0.0071],\n",
            "        [0.0064, 0.0023, 0.0165,  ..., 0.0077, 0.0094, 0.0041],\n",
            "        [0.0108, 0.0103, 0.0093,  ..., 0.0091, 0.0069, 0.0075]],\n",
            "       device='cuda:0')\n",
            "['00000.jpg', '00001.jpg', '00002.jpg', '00003.jpg', '00004.jpg', '00005.jpg', '00006.jpg', '00007.jpg', '00008.jpg', '00009.jpg']\n",
            "['01999.jpg', '01998.jpg', '01997.jpg', '01996.jpg', '01995.jpg', '01994.jpg', '01993.jpg', '01992.jpg', '01991.jpg', '01990.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get_transport_matrices(centroids, weights, total_samples=100_000, batch_size = 32768, epsilon=.1, max_iters=10000)\n",
        "# should run with high RAM on\n",
        "def get_transport_matrices(centroids, weights, total_samples=100_000, batch_size = 32768, epsilon=.1, max_iters=10000):\n",
        "  '''\n",
        "  Generates the P transport matrices using batched sinkhorn\n",
        "  Parameters:\n",
        "    centroids: shape (batch, k, 3)\n",
        "    weights: shape (batch, k)\n",
        "    total_samples: number of samples to generate\n",
        "    batch_size: size of each batch to send to gpu\n",
        "    epsilon and max_iters: params for sinkhorn\n",
        "  Returns:\n",
        "    X_src_indices: (total_samples,) Indices used from centroids and weights\n",
        "    X_tgt_indices: (total_samples,) Indices used from centroids and weights\n",
        "    y_transport_matrices: (total_samples, k, k)\n",
        "  '''\n",
        "  num_batches = (total_samples + batch_size - 1) // batch_size\n",
        "\n",
        "  X_src_indices = []\n",
        "  X_tgt_indices = []\n",
        "  y_transport_matrices = []\n",
        "\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    curr_batch_size = min(batch_size, total_samples - i * batch_size)\n",
        "    source_indices = torch.randint(low=0, high=centroids.shape[0], size=(curr_batch_size,))\n",
        "    target_indices = torch.randint(low=0, high=centroids.shape[0], size=(curr_batch_size,))\n",
        "\n",
        "    centroids_batch_a = centroids[source_indices]\n",
        "    centroids_batch_b = centroids[target_indices]\n",
        "    weights_batch_a = weights[source_indices]\n",
        "    weights_batch_b = weights[target_indices]\n",
        "\n",
        "    batch_cost_matrix = torch.cdist(centroids_batch_a, centroids_batch_b) # shape (batch, k, k)\n",
        "\n",
        "    # get transport matrix batch\n",
        "    with torch.no_grad():\n",
        "      batch_P = sinkhorn_batch(weights_batch_a, weights_batch_b, batch_cost_matrix, epsilon=epsilon, max_iters=max_iters)\n",
        "    X_src_indices.append(source_indices.cpu())\n",
        "    X_tgt_indices.append(target_indices.cpu())\n",
        "    y_transport_matrices.append(batch_P.cpu())\n",
        "\n",
        "  X_src_indices = torch.cat(X_src_indices) # cast along dim=0\n",
        "  X_tgt_indices = torch.cat(X_tgt_indices)\n",
        "  y_transport_matrices = torch.cat(y_transport_matrices)\n",
        "\n",
        "  return X_src_indices, X_tgt_indices, y_transport_matrices"
      ],
      "metadata": {
        "id": "866lUuJ2FVZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_sinkhorn_results_locally():\n",
        "  # to save space we will save the indices of the source/target centroids/weights\n",
        "  output_dir = '/content/sinkhorn_inputs_matrices/'\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  X_src_ind, X_tgt_ind, y_trans_mats = get_transport_matrices(centroids_tensor, weights_tensor)\n",
        "  torch.save({\n",
        "    'X_src_indices': X_src_ind,\n",
        "    'X_tgt_indices': X_tgt_ind,\n",
        "    'y_transport_matrices': y_trans_mats\n",
        "  }, os.path.join(output_dir, 'sinkhorn_bank.pt'))\n",
        "\n",
        "\n",
        "def save_sinkhorn_results_to_drive():\n",
        "  drive_dir = '/content/drive/MyDrive/Amortized Optimal Transport/Data'\n",
        "\n",
        "  print(\"Zipping Sinkhorn results...\")\n",
        "  archive_path = shutil.make_archive(base_name=f'/content/sinkhorn_bank', format='zip', root_dir='/content/sinkhorn_inputs_matrices/')\n",
        "  print(\"Copying sinkhorn results to drive...\")\n",
        "  try:\n",
        "    shutil.copy(archive_path, drive_dir)\n",
        "    print(\"Done\")\n",
        "  except Exception as e:\n",
        "    print(f\"error: {e}\")"
      ],
      "metadata": {
        "id": "mDGbyBm3t_6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_sinkhorn_results_locally()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oUhlXts7Tj64",
        "outputId": "732ed82b-93d1-4459-9c7e-eee1a5ddb244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [1:50:28<00:00, 1657.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_sinkhorn_results_to_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBCXEHkX0Jop",
        "outputId": "b09f94c3-17a4-45df-d208-dc841d78b1f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping Sinkhorn results...\n",
            "Copying sinkhorn results to drive...\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}